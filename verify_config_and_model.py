#!/usr/bin/env python3


import sys
import os
import yaml
import torch
sys.path.append('/home/szk_25/FedSA-LoRA')
sys.path.append('/home/szk_25/FedSA-LoRA-Dual')

from federatedscope.core.configs.config import CN
from federatedscope.core.cmd_args import parse_args
from dual_lora_model_builder import get_dual_lora_llm

def verify_config_and_model(config_path='/home/szk_25/FedSA-LoRA-Dual/dual_lora_config.yaml'):
    """éªŒè¯é…ç½®å’Œæ¨¡å‹æ˜¯å¦æ­£ç¡®è®¾ç½®"""
    print("=" * 60)
    print("éªŒè¯é…ç½®å’Œæ¨¡å‹è®¾ç½®")
    print("=" * 60)
    
    # 1. è¯»å–YAMLé…ç½®
    print("\n1. è¯»å–é…ç½®æ–‡ä»¶...")
    with open(config_path, 'r') as f:
        yaml_cfg = yaml.safe_load(f)
    
    print(f"   Method: {yaml_cfg['federate']['method']}")
    print(f"   Data num_labels (YAML): {yaml_cfg['data'].get('num_labels', 'Not set')}")
    
    # 2. åŠ è½½FederatedScopeé…ç½®
    print("\n2. åŠ è½½FederatedScopeé…ç½®...")
    # ä½¿ç”¨parse_argsåŠ è½½é…ç½®ï¼ˆæ¨¡æ‹Ÿå‘½ä»¤è¡Œå‚æ•°ï¼‰
    import sys
    original_argv = sys.argv
    sys.argv = ['verify_config_and_model.py', '--cfg', config_path]
    try:
        from federatedscope.core.configs.config import global_cfg
        args = parse_args()
        cfg = global_cfg.clone()
        cfg.merge_from_file(config_path)
    finally:
        sys.argv = original_argv
    
    print(f"   Method: {cfg.federate.method}")
    print(f"   Data num_labels: {getattr(cfg.data, 'num_labels', 'Not set')}")
    
    # ç¡®ä¿num_labelsè¢«è®¾ç½®
    if not hasattr(cfg.data, 'num_labels') or cfg.data.num_labels is None:
        cfg.data.num_labels = 3
        print(f"   âš ï¸  num_labelsæœªè®¾ç½®ï¼Œè®¾ç½®ä¸ºé»˜è®¤å€¼: 3")
    
    # 3. åŠ è½½æ•°æ®é›†ä»¥éªŒè¯æ ‡ç­¾
    print("\n3. éªŒè¯æ•°æ®é›†æ ‡ç­¾...")
    try:
        from federatedscope.glue.dataloader.dataloader import load_glue_dataset
        dataset, updated_config = load_glue_dataset(cfg)
        train_dataset, eval_dataset, test_dataset = dataset
        
        if hasattr(train_dataset, 'label'):
            train_labels = train_dataset['label']
            min_label = train_labels.min().item() if hasattr(train_labels, 'min') else min(train_labels)
            max_label = train_labels.max().item() if hasattr(train_labels, 'max') else max(train_labels)
            print(f"   è®­ç»ƒé›†æ ‡ç­¾èŒƒå›´: {min_label} - {max_label}")
            
            if max_label >= cfg.data.num_labels:
                print(f"   âŒ é”™è¯¯: æ ‡ç­¾å€¼ {max_label} >= num_labels {cfg.data.num_labels}")
                print(f"   ğŸ’¡ å»ºè®®: ç¡®ä¿num_labelsè®¾ç½®ä¸º {max_label + 1} æˆ–æ›´å¤§")
            else:
                print(f"   âœ… æ ‡ç­¾èŒƒå›´åœ¨æœ‰æ•ˆèŒƒå›´å†… [0, {cfg.data.num_labels})")
        else:
            print("   âš ï¸  æ— æ³•è®¿é—®æ•°æ®é›†æ ‡ç­¾")
            
    except Exception as e:
        print(f"   âš ï¸  æ— æ³•åŠ è½½æ•°æ®é›†: {e}")
    
    # 4. åˆ›å»ºæ¨¡å‹å¹¶éªŒè¯åˆ†ç±»å™¨
    print("\n4. éªŒè¯æ¨¡å‹åˆ†ç±»å™¨...")
    try:
        model = get_dual_lora_llm(cfg)
        
        # æŸ¥æ‰¾åˆ†ç±»å™¨å±‚
        classifier = None
        classifier_name = None
        
        # æ£€æŸ¥ä¸åŒçš„å¯èƒ½åˆ†ç±»å™¨åç§°
        for name, module in model.named_modules():
            if hasattr(module, 'out_features') or name in ['classifier', 'score', 'head']:
                if hasattr(module, 'weight'):
                    classifier = module
                    classifier_name = name
                    break
        
        if classifier is None:
            # å°è¯•ä»modelçš„å±æ€§è·å–
            for attr_name in ['classifier', 'score', 'head']:
                if hasattr(model, attr_name):
                    classifier = getattr(model, attr_name)
                    classifier_name = attr_name
                    break
        
        if classifier is None:
            print("   âš ï¸  æœªæ‰¾åˆ°åˆ†ç±»å™¨å±‚")
        else:
            if hasattr(classifier, 'out_features'):
                model_num_labels = classifier.out_features
                print(f"   åˆ†ç±»å™¨å±‚ ({classifier_name}): out_features = {model_num_labels}")
                
                if model_num_labels != cfg.data.num_labels:
                    print(f"   âŒ é”™è¯¯: æ¨¡å‹åˆ†ç±»å™¨è¾“å‡ºç»´åº¦ ({model_num_labels}) != é…ç½®çš„num_labels ({cfg.data.num_labels})")
                    print(f"   ğŸ’¡ å»ºè®®: ç¡®ä¿é…ç½®ä¸­çš„num_labelsè®¾ç½®ä¸º {model_num_labels}")
                else:
                    print(f"   âœ… æ¨¡å‹åˆ†ç±»å™¨è¾“å‡ºç»´åº¦åŒ¹é…é…ç½®: {model_num_labels}")
            else:
                print(f"   âš ï¸  åˆ†ç±»å™¨å±‚ {classifier_name} æ²¡æœ‰ out_features å±æ€§")
                print(f"   åˆ†ç±»å™¨æƒé‡å½¢çŠ¶: {classifier.weight.shape if hasattr(classifier, 'weight') else 'N/A'}")
        
        # 5. æµ‹è¯•å‰å‘ä¼ æ’­
        print("\n5. æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
        try:
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
            batch_size = 2
            seq_len = 128
            input_ids = torch.randint(1, 1000, (batch_size, seq_len))
            attention_mask = torch.ones(batch_size, seq_len)
            labels = torch.randint(0, cfg.data.num_labels, (batch_size,))
            
            print(f"   è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
            print(f"   æ ‡ç­¾å€¼: {labels.tolist()}")
            
            with torch.no_grad():
                outputs = model(input_ids=input_ids, 
                              attention_mask=attention_mask,
                              labels=labels)
            
            if hasattr(outputs, 'logits'):
                print(f"   âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
                print(f"   Logitså½¢çŠ¶: {outputs.logits.shape}")
                print(f"   é¢„æœŸå½¢çŠ¶: ({batch_size}, {cfg.data.num_labels})")
                
                if outputs.logits.shape[1] != cfg.data.num_labels:
                    print(f"   âŒ é”™è¯¯: Logitsè¾“å‡ºç»´åº¦ä¸åŒ¹é…")
                else:
                    print(f"   âœ… Logitsè¾“å‡ºç»´åº¦åŒ¹é…")
            else:
                print(f"   âš ï¸  è¾“å‡ºæ²¡æœ‰logitså±æ€§")
                
        except Exception as e:
            print(f"   âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
    except Exception as e:
        print(f"   âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "=" * 60)
    print("éªŒè¯å®Œæˆ")
    print("=" * 60)

if __name__ == "__main__":
    verify_config_and_model()

