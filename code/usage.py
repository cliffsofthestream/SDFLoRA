

import torch
import torch.nn as nn
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
CURRENT_PROJECT_PATH = "/home/user/FedSA-LoRA-Dual"
ORIGINAL_PROJECT_PATH = "/home/user/FedSA-LoRA"

if CURRENT_PROJECT_PATH not in sys.path:
    sys.path.insert(0, CURRENT_PROJECT_PATH)
if ORIGINAL_PROJECT_PATH not in sys.path:
    sys.path.insert(0, ORIGINAL_PROJECT_PATH)

from code.dual_lora_adapter import create_dual_lora_model, DualLoRAConfig
from code.dual_lora_peft_adapter import enable_dual_lora_adapter, DualLoraConfig
from code.dual_lora_aggregator import DualLoRAAggregator
from code.dual_lora_model_builder import DualLoRAModelBuilder

import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def example_1_basic_usage():
    """ç¤ºä¾‹1: åŸºç¡€ä½¿ç”¨æ–¹æ³•"""
    logger.info("=== ç¤ºä¾‹1: åŸºç¡€ä½¿ç”¨æ–¹æ³• ===")
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„åŸºç¡€æ¨¡å‹
    class SimpleTransformer(nn.Module):
        def __init__(self, hidden_dim=768, num_classes=2):
            super().__init__()
            self.embedding = nn.Embedding(30000, hidden_dim)
            self.query = nn.Linear(hidden_dim, hidden_dim)
            self.key = nn.Linear(hidden_dim, hidden_dim)
            self.value = nn.Linear(hidden_dim, hidden_dim)
            self.dense = nn.Linear(hidden_dim, hidden_dim)
            self.classifier = nn.Linear(hidden_dim, num_classes)
        
        def forward(self, input_ids):
            x = self.embedding(input_ids)
            # ç®€åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶
            q = self.query(x)
            k = self.key(x)
            v = self.value(x)
            # ç®€åŒ–å¤„ç†
            attn_output = self.dense(v)
            # æ± åŒ–å¹¶åˆ†ç±»
            pooled = attn_output.mean(dim=1)
            return self.classifier(pooled)
    
    # åˆ›å»ºåŸºç¡€æ¨¡å‹
    base_model = SimpleTransformer()
    logger.info(f"Base model parameters: {sum(p.numel() for p in base_model.parameters()):,}")
    
    # åº”ç”¨åŒæ¨¡å—LoRA
    dual_lora_model = enable_dual_lora_adapter(
        model=base_model,
        global_r=8,
        local_r=4,
        lora_alpha=16,
        fusion_method="weighted_sum",
        target_modules=["query", "key", "value", "dense"]
    )
    
    # æ‰“å°å‚æ•°ä¿¡æ¯
    dual_lora_model.print_trainable_parameters()
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    input_ids = torch.randint(0, 30000, (2, 10))
    output = dual_lora_model(input_ids)
    logger.info(f"Output shape: {output.shape}")
    
    # è·å–å…¨å±€å’Œæœ¬åœ°å‚æ•°
    global_params = dual_lora_model.get_global_state_dict()
    local_params = dual_lora_model.get_local_state_dict()
    
    logger.info(f"Global parameters: {len(global_params)} tensors")
    logger.info(f"Local parameters: {len(local_params)} tensors")
    
    return dual_lora_model


def example_2_federated_aggregation():
    """ç¤ºä¾‹2: è”é‚¦èšåˆè¿‡ç¨‹"""
    logger.info("\n=== ç¤ºä¾‹2: è”é‚¦èšåˆè¿‡ç¨‹ ===")
    
    # æ¨¡æ‹Ÿ3ä¸ªå®¢æˆ·ç«¯çš„åŒæ¨¡å—LoRAå‚æ•°
    client_models = []
    
    for client_id in range(1, 4):
        # æ¯ä¸ªå®¢æˆ·ç«¯æœ‰ä¸åŒçš„ranké…ç½®
        global_r = 8 if client_id == 1 else (6 if client_id == 2 else 4)
        local_r = 4 if client_id == 1 else (6 if client_id == 2 else 8)
        
        logger.info(f"Client {client_id}: global_r={global_r}, local_r={local_r}")
        
        # æ¨¡æ‹Ÿå®¢æˆ·ç«¯å‚æ•°
        client_params = {
            f"query.global_lora_A.weight": torch.randn(global_r, 768),
            f"query.global_lora_B.weight": torch.randn(768, global_r),
            f"query.local_lora_A.weight": torch.randn(local_r, 768),
            f"query.local_lora_B.weight": torch.randn(768, local_r),
            f"query.global_weight": torch.tensor(0.7),
            f"query.local_weight": torch.tensor(0.3),
            f"classifier.weight": torch.randn(2, 768),
            f"classifier.bias": torch.randn(2),
        }
        client_models.append(client_params)
    
    # åˆ›å»ºèšåˆå™¨
    aggregator = DualLoRAAggregator(
        global_aggregation_strategy="stacked",  # ä½¿ç”¨å †å èšåˆæ”¯æŒå¼‚æ„
        local_personalization_strategy="local_only",
        client_ranks={1: (8, 4), 2: (6, 6), 3: (4, 8)},
        enable_stacking=True,
        enable_heterogeneous=True
    )
    
    # å‡†å¤‡èšåˆä¿¡æ¯
    agg_info = {
        "client_feedback": [
            (1, (100, client_models[0])),  # (client_id, (sample_size, model_params))
            (2, (150, client_models[1])),
            (3, (120, client_models[2]))
        ]
    }
    
    # æ‰§è¡Œèšåˆ
    aggregated_params = aggregator.aggregate(agg_info)
    
    logger.info(f"Aggregated parameters: {len(aggregated_params)} tensors")
    
    # æ˜¾ç¤ºèšåˆåçš„å…¨å±€å‚æ•°å½¢çŠ¶
    for key, value in aggregated_params.items():
        if "global_lora" in key:
            logger.info(f"{key}: {value.shape}")
    
    return aggregated_params


def example_3_heterogeneous_clients():
    """ç¤ºä¾‹3: å¼‚æ„å®¢æˆ·ç«¯é…ç½®"""
    logger.info("\n=== ç¤ºä¾‹3: å¼‚æ„å®¢æˆ·ç«¯é…ç½® ===")
    
    # å®šä¹‰ä¸åŒç±»å‹çš„å®¢æˆ·ç«¯é…ç½®
    client_configs = {
        "high_resource": {"global_r": 16, "local_r": 4},      # é«˜èµ„æºå®¢æˆ·ç«¯
        "balanced": {"global_r": 8, "local_r": 8},            # å¹³è¡¡å®¢æˆ·ç«¯
        "personalized": {"global_r": 4, "local_r": 16},       # é«˜ä¸ªæ€§åŒ–å®¢æˆ·ç«¯
        "limited": {"global_r": 4, "local_r": 4},             # èµ„æºå—é™å®¢æˆ·ç«¯
    }
    
    models = {}
    
    for client_type, config in client_configs.items():
        logger.info(f"Creating {client_type} client model...")
        
        # åˆ›å»ºåŸºç¡€æ¨¡å‹
        base_model = nn.Sequential(
            nn.Linear(768, 768),
            nn.ReLU(),
            nn.Linear(768, 2)
        )
        
        # åº”ç”¨åŒæ¨¡å—LoRA
        dual_model = enable_dual_lora_adapter(
            model=base_model,
            global_r=config["global_r"],
            local_r=config["local_r"],
            fusion_method="gating",  # ä½¿ç”¨é—¨æ§èåˆ
            target_modules=["0", "2"]  # ç›®æ ‡çº¿æ€§å±‚
        )
        
        models[client_type] = dual_model
        dual_model.print_trainable_parameters()
    
    return models


def example_4_fusion_methods_comparison():
    """ç¤ºä¾‹4: èåˆæ–¹æ³•æ¯”è¾ƒ"""
    logger.info("\n=== ç¤ºä¾‹4: èåˆæ–¹æ³•æ¯”è¾ƒ ===")
    
    fusion_methods = ["weighted_sum", "gating", "attention"]
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, hidden_dim = 4, 20, 768
    x = torch.randn(batch_size, seq_len, hidden_dim)
    
    results = {}
    
    for method in fusion_methods:
        logger.info(f"Testing fusion method: {method}")
        
        # åˆ›å»ºåŸºç¡€æ¨¡å‹
        base_model = nn.Linear(hidden_dim, hidden_dim)
        
        # åº”ç”¨åŒæ¨¡å—LoRA
        dual_model = enable_dual_lora_adapter(
            model=base_model,
            global_r=8,
            local_r=4,
            fusion_method=method,
            target_modules=[""]  # åº”ç”¨åˆ°æ•´ä¸ªæ¨¡å‹
        )
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output = dual_model(x)
        
        results[method] = {
            "output_shape": output.shape,
            "output_mean": output.mean().item(),
            "output_std": output.std().item()
        }
        
        logger.info(f"  Output shape: {output.shape}")
        logger.info(f"  Output mean: {output.mean().item():.4f}")
        logger.info(f"  Output std: {output.std().item():.4f}")
    
    return results


def example_5_parameter_analysis():
    """ç¤ºä¾‹5: å‚æ•°åˆ†æ"""
    logger.info("\n=== ç¤ºä¾‹5: å‚æ•°åˆ†æ ===")
    
    # åˆ›å»ºä¸åŒé…ç½®çš„æ¨¡å‹è¿›è¡Œæ¯”è¾ƒ
    configs = [
        {"name": "Small", "global_r": 4, "local_r": 2},
        {"name": "Medium", "global_r": 8, "local_r": 4},
        {"name": "Large", "global_r": 16, "local_r": 8},
        {"name": "Global-focused", "global_r": 16, "local_r": 2},
        {"name": "Local-focused", "global_r": 4, "local_r": 16},
    ]
    
    base_model = nn.Sequential(
        nn.Linear(768, 768),
        nn.Linear(768, 768),
        nn.Linear(768, 2)
    )
    
    base_params = sum(p.numel() for p in base_model.parameters())
    logger.info(f"Base model parameters: {base_params:,}")
    
    for config in configs:
        dual_model = enable_dual_lora_adapter(
            model=nn.Sequential(
                nn.Linear(768, 768),
                nn.Linear(768, 768),
                nn.Linear(768, 2)
            ),
            global_r=config["global_r"],
            local_r=config["local_r"],
            target_modules=["0", "1"]
        )
        
        global_params = sum(p.numel() for p in dual_model.get_global_state_dict().values())
        local_params = sum(p.numel() for p in dual_model.get_local_state_dict().values())
        total_trainable = global_params + local_params
        
        logger.info(f"{config['name']} configuration:")
        logger.info(f"  Global params: {global_params:,}")
        logger.info(f"  Local params: {local_params:,}")
        logger.info(f"  Total trainable: {total_trainable:,}")
        logger.info(f"  Efficiency: {total_trainable/base_params*100:.2f}%")


def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    logger.info("åŒæ¨¡å—LoRAä½¿ç”¨ç¤ºä¾‹")
    logger.info("=" * 50)
    
    try:
        # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
        model1 = example_1_basic_usage()
        agg_params = example_2_federated_aggregation()
        hetero_models = example_3_heterogeneous_clients()
        fusion_results = example_4_fusion_methods_comparison()
        example_5_parameter_analysis()
        
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡ŒæˆåŠŸï¼")
        logger.info("åŒæ¨¡å—LoRAå®ç°éªŒè¯å®Œæˆã€‚")
        
        return True
        
    except Exception as e:
        logger.error(f"ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
