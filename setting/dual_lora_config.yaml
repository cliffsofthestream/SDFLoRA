# 双模块LoRA配置文件（带DP-SGD支持）
# 参考: NeurIPS 2024 - Dual-Personalizing Adapter for Federated Foundation Models
# 
# 核心思想：
# 1. 全局适配器：处理跨客户端共享的通用知识，参与联邦聚合
# 2. 本地适配器：处理客户端特定的个性化需求，保持本地
# 3. DP-SGD：为全局适配器提供差分隐私保护

seed: 789
use_gpu: True
device: 0
early_stop:
  patience: 0

# DP-SGD配置
differential_privacy:
  enabled: True
  epsilon: 2.0             # 隐私预算
  delta: 1e-4              # 失败概率
  max_grad_norm: 1.0       # 梯度裁剪阈值
  noise_multiplier: 1.1    # 噪声乘数（自动计算）
  
  # 双模块LoRA特定配置
  apply_to_global: True     # 对全局适配器应用DP-SGD
  apply_to_local: False    # 对本地适配器应用DP-SGD（通常保持本地）
  global_noise_scale: 1.0  # 全局适配器噪声缩放
  local_noise_scale: 0.5   # 本地适配器噪声缩放
  
  # 联邦学习隐私配置
  enable_secure_aggregation: True  # 启用安全聚合
  aggregation_noise_scale: 0.8    # 聚合噪声缩放

federate:
  # 双模块LoRA特定配置
  freeze_A: False           # 是否冻结A矩阵（对应全局适配器）
  mode: standalone
  client_num: 8
  total_round_num: 12
  save_to: "dual_lora_mnli.ckpt"
  share_local_model: True
  online_aggr: False
  method: "dual-lora"       # 使用双模块LoRA方法

# 个性化配置 - 双模块LoRA中本地参数保持本地
personalization: 
  local_param: ['local_lora_A', 'local_lora_B', 'global_weight', 'local_weight', 'gate', 'classifier']

# 聚合器配置 - 支持双模块LoRA聚合
aggregator:
  # 鲁棒性规则 - 使用双模块LoRA堆叠聚合
  robust_rule: 'dual_lora_stacked'
  
  # 堆叠和异构支持（继承原有FedSA-LoRA功能）
  stacking: True
  zero_padding: False
  heter: True
  
  # 客户端权重缩放因子配置（论文中的pk参数）
  client_scaling_factor: 0.1  # 客户端贡献权重缩放因子，论文测试了0.01, 0.05, 0.1, 0.2
  use_fixed_scaling: True    # 是否使用固定缩放因子而非基于数据量的权重
  
  # 异构客户端rank配置（使用原项目格式）
  local_ranks:
    "0": 8    
    "1": 8    
    "2": 8   
    "3": 8
    "4": 8
    "5": 8
    "6": 8
    "7": 8


data:
  root: /home/szk_25/FedSA-LoRA-Dual/GLUE  # 使用本地GLUE数据集路径
  type: 'mnli@glue'  # 使用MNLI数据集
  matched: True
  splitter: 'lda'
  splitter_args: [{'alpha': 0.5}]
  num_labels: 3  # MNLI数据集有3个标签（contradiction, neutral, entailment）

llm:
  tok_len: 128
  adapter:
    use: True
    args: 
      - adapter_package: 'dual_lora'
        adapter_method: 'dual_lora'
        use_dual_lora: True
        
        # 全局适配器配置
        global_r: 8
        lora_alpha: 16
        lora_dropout: 0.05
        
        # 本地适配器配置  
        local_r: 4
        
        # 融合机制配置
        fusion_method: 'weighted_sum'  # 'weighted_sum', 'gating', 'attention'
        
        # 目标模块
        target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']

dataloader:
  batch_size: 8

model:
  type: '/home/szk_25/FederatedLLM/llama-7b@huggingface_llm'

train:
  local_update_steps: 10
  batch_or_epoch: batch
  optimizer:
    lr: 2e-4
  is_enable_half: True

criterion:
  type: CrossEntropyLoss

trainer:
  type: gluetrainer

eval:
  freq: 1
  metrics: ['accuracy', 'f1']
  count_flops: False
  best_res_update_round_wise_key: 'val_accuracy'
